{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc0e1dd-67e8-4bd1-ad02-2a50e3087c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1 :\n",
    "# GridSearchCV is a Cross validation technique\n",
    "# purpose of  Cross Validation is hyperparameter tunig of model\n",
    "# Main Aim of hyperparameter tuning is to find the best parametrs which helps to increase accuracy\n",
    "# Grid search cross-validation (GridSearchCV) is a technique used in machine learning to find the \n",
    "# best combination of hyperparameters for a given model.\n",
    "\n",
    "# The purpose of GridSearchCV is to automate the process of hyperparameter tuning, which involves selecting the values for hyperparameters\n",
    "# that yield the best performance for a specific model\n",
    "\n",
    "# working\n",
    "# 1.Define the Model and Hyperparameter Grid: Specify the machine learning model to be tuned and the hyperparameters to be optimized.\n",
    "# 2.Cross-Validation: Choose a cross-validation strategy, typically k-fold cross-validation. \n",
    "#   This involves splitting the training data into k subsets (folds)\n",
    "# 3.Grid Search: GridSearchCV exhaustively searches through all possible combinations of hyperparameter values defined in the grid\n",
    "# 4.Select the Best Hyperparameters: GridSearchCV identifies the set of hyperparameters that \n",
    "#   yielded the best performance based on the chosen performance metric\n",
    "# 5.Retrain the Model with Best Hyperparameters: Once the best hyperparameters are identified, \n",
    "#   the model is retrained using the entire training dataset with the optimal hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "252dcc46-2560-4d88-bf89-4f6ebb2c1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2 :\n",
    "# # GridSearchCV is a Cross validation technique\n",
    "# purpose of  Cross Validation is hyperparameter tunig of model\n",
    "# Main Aim of hyperparameter tuning is to find the best parametrs which helps to increase accuracy\n",
    "# Grid search cross-validation (GridSearchCV) is a technique used in machine learning to find the \n",
    "# best combination of hyperparameters for a given model.\n",
    "# It explores the entire hyperparameter grid, covering all possible combinations.\n",
    "# As it has a disadvantage because it checks all parameters combination \n",
    "# time complexity will be more for the training model\n",
    "# that's we use another technique called RandomizedSearch CV\n",
    "\n",
    "# \"RandomizedSearchCV\"\n",
    "# Randomized is also a Cross validation technique\n",
    "# Main Aim of hyperparameter tuning is to find the best parametrs which helps to increase accuracy\n",
    "# It randomly samples a specified number of combinations from the hyperparameter space. \n",
    "# Instead of trying every possible combination, it selects combinations at random.\n",
    "# It randomly samples a subset of the hyperparameter space, defined by the number of iterations.\n",
    "# It is more computationally efficient because it randomly samples a subset of the hyperparameter space.\n",
    "\n",
    "# We will go Randomized SearchCV because the time complexity will less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cab58a-4e58-4716-a718-449143e0aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3 :\n",
    "# Data leakage, also known as information leakage, \n",
    "# occurs when information from the future or outside the training dataset is unintentionally used during the model development process. \n",
    "\n",
    "# Data leakage is a significant problem in machine learning because \n",
    "# it can result in overly optimistic performance estimates, making the model appear better than it actually is.\n",
    "\n",
    "# for example\n",
    "# Suppose you are building a credit card fraud detection model. \n",
    "# In your dataset, you have features related to each transaction, such as the transaction amount, merchant ID, and timestamp. \n",
    "# The target variable indicates whether the transaction is fraudulent or not. \n",
    "# However, you also have an additional feature called \"is_fraudulent\" that indicates \n",
    "# whether a transaction is fraudulent or not, which is generated based on future information not available at the time of prediction.\n",
    "# If you include this \"is_fraudulent\" feature in the training process, \n",
    "# the model will have access to information from the future, resulting in inflated performance during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b46dba0-656e-45b9-88f8-4e82d1e986e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4 :\n",
    "# 1.Understand the Problem and Data: Gain a thorough understanding of the problem domain, the data, \n",
    "#   and the temporal or conditional relationships within the data. \n",
    "# 2.Separate Training and Test Data: Split your dataset into distinct training and test sets. \n",
    "#   Ensure that the test set represents new, unseen data that is independent of the training process.\n",
    "# 3.Avoid Future Information: Be cautious not to include any features, variables, or \n",
    "#    information that would not be available at the time of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259f95c1-9194-4e84-aaa3-98cf069efedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5 :\n",
    "# Confusion Matrix\n",
    "# It is a 2x2 Matrix with actual values and predicted values\n",
    "# A confusion matrix is a table that summarizes the performance of a classification model by \n",
    "# displaying the counts of true positive, true negative, false positive, and false negative predictions. \n",
    "# It is a useful tool for evaluating the performance of a classification model \n",
    "#                Predicted\n",
    "#                     Positive   Negative\n",
    "#   Actual   Positive    TP         FN\n",
    "#            Negative    FP         TN\n",
    "\n",
    "# where\n",
    "# TP (True Positive) represents the number of instances correctly predicted as positive by the model.\n",
    "# TN (True Negative) represents the number of instances correctly predicted as negative by the model.\n",
    "# FP (False Positive) represents the number of instances incorrectly predicted as positive when they are actually negative. \n",
    "# FN (False Negative) represents the number of instances incorrectly predicted as negative when they are actually positive.\n",
    "\n",
    "# It tells about the \n",
    "# 1.Accuracy: The overall accuracy of the model can be calculated as (TP + TN) / (TP + TN + FP + FN). \n",
    "#   It measures the proportion of correctly classified instances out of the total.\n",
    "# 2.Precision: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "#   It is calculated as TP / (TP + FP)\n",
    "# 3.Recall: Recall measures the proportion of correctly predicted positive instances out of all actual positive instances. \n",
    "#          It is calculated as TP / (TP + FN). \n",
    "# 4.F1 Score: The F1 score is the harmonic mean of precision and recall.\n",
    "#   It provides a single metric that balances both precision and recall. \n",
    "#   It is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "# The confusion matrix allows you to assess the performance of a classification model \n",
    "# in terms of its ability to correctly classify instances into different classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf017c4-51ad-475e-a1f5-339cc482d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6 :\n",
    "# 1.Precision: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "#   It is calculated as TP / (TP + FP)\n",
    "#   we use precesion when False Positive is important\n",
    "# 2.Recall: Recall measures the proportion of correctly predicted positive instances out of all actual positive instances. \n",
    "#          It is calculated as TP / (TP + FN).\n",
    "#   we use Recall when  false negtive important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66da844-edd5-4495-9ca9-333e1e9c0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7 :\n",
    "# Here's how you can interpret the confusion matrix\n",
    "# 1. TP (True Positive) represents the number of instances correctly predicted as positive by the model.\n",
    "#    for example : a true positive would be when the model correctly predicts a disease in a patient who actually has the disease.\n",
    "# 2.TN (True Negative) represents the number of instances correctly predicted as negative by the model.\n",
    "#      for example: a true negative would be when the model correctly identifies a non-spam email as non-spam.\n",
    "# 3.FP (False Positive) represents the number of instances incorrectly predicted as positive when they are actually negative.\n",
    "#     for example : a false positive would be when the model flags a legitimate transaction as fraudulent.\n",
    "# 4.FN (False Negative) represents the number of instances incorrectly predicted as negative when they are actually positive.\n",
    "#     for example :a false negative would be when the model fails to identify a cancerous tumor in a patient who actually has cancer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b272a2-9bbb-4dde-93b7-bee3173cba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 8 :\n",
    "# 1.Accuracy: The overall accuracy of the model can be calculated as (TP + TN) / (TP + TN + FP + FN). \n",
    "#   It measures the proportion of correctly classified instances out of the total.\n",
    "# 2.Precision: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "#   It is calculated as TP / (TP + FP)\n",
    "# 3.Recall: Recall measures the proportion of correctly predicted positive instances out of all actual positive instances. \n",
    "#          It is calculated as TP / (TP + FN). \n",
    "# 4.F1 Score: The F1 score is the harmonic mean of precision and recall.\n",
    "#   It provides a single metric that balances both precision and recall. \n",
    "#   It is calculated as 2 * (Precision * Recall) / (Precision + Recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68375b13-f0b5-4790-9273-7d4060c71d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 9 :\n",
    "# Realtion between accuracy and values in confusion matrix is :\n",
    "# confusion consists of 4 terms\n",
    "# TP (True Positive) represents the number of instances correctly predicted as positive by the model.\n",
    "# TN (True Negative) represents the number of instances correctly predicted as negative by the model.\n",
    "# FP (False Positive) represents the number of instances incorrectly predicted as positive when they are actually negative. \n",
    "# FN (False Negative) represents the number of instances incorrectly predicted as negative when they are actually positive.\n",
    "\n",
    "# 1.Accuracy: The overall accuracy of the model can be calculated as (TP + TN) / (TP + TN + FP + FN). \n",
    "#   It measures the proportion of correctly classified instances out of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff926988-ea9f-42bb-9c3d-2db75805e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 10 :\n",
    "# A confusion matrix can help identify potential biases or limitations in a machine learning model by providing insights \n",
    "# into the model's performance across different classes.\n",
    "# 1.Class Imbalance: Check if there is a significant difference in the number of instances between different classes. \n",
    "#   If one class dominates the dataset while others are underrepresented, it can lead to biased predictions.\n",
    "# 2.Misclassification Patterns: Analyze the distribution of false positives and false negatives across different classes. \n",
    "#   Look for consistent patterns in misclassifications.\n",
    "# 3.Domain Knowledge: Use domain knowledge to interpret the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da493a61-aed4-48f7-ae71-03fb97eb086b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
